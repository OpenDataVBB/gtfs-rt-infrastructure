# NATS setup: streams, consumers
nats_ref_aus_sollfahrt_stream_name: REF_AUS_SOLLFAHRT_2
nats_ref_aus_sollfahrt_stream_description: 'VDV-453 REF-AUS SollFahrt messages'
nats_ref_aus_sollfahrt_stream_subjects:
  - 'ref_aus.sollfahrt.>'
nats_ref_aus_sollfahrt_stream_max_size_gb: 20

nats_ref_aus_sollfahrt_consumer_name: gtfs-rt-feed
nats_ref_aus_sollfahrt_consumer_description: 'OpenDataVBB/gtfs-rt-feed'
nats_ref_aus_sollfahrt_consumer_deliver: new
nats_ref_aus_sollfahrt_consumer_max_pending: 100
nats_ref_aus_sollfahrt_consumer_max_deliver: 3
nats_ref_aus_sollfahrt_consumer_backoff: linear
nats_ref_aus_sollfahrt_consumer_backoff_steps: 2
nats_ref_aus_sollfahrt_consumer_backoff_min: 1m
nats_ref_aus_sollfahrt_consumer_backoff_max: 5m

nats_aus_istfahrt_stream_name: AUS_ISTFAHRT_2
nats_aus_istfahrt_stream_description: 'VDV-454 AUS IstFahrt messages'
nats_aus_istfahrt_stream_subjects:
  - 'aus.istfahrt.>'
nats_aus_istfahrt_stream_max_size_gb: 20

nats_aus_istfahrt_consumer_name: gtfs-rt-feed
nats_aus_istfahrt_consumer_description: 'OpenDataVBB/gtfs-rt-feed'
nats_aus_istfahrt_consumer_deliver: new
nats_aus_istfahrt_consumer_max_pending: 100
nats_aus_istfahrt_consumer_max_deliver: 3
nats_aus_istfahrt_consumer_backoff: linear
nats_aus_istfahrt_consumer_backoff_steps: 2
nats_aus_istfahrt_consumer_backoff_min: 1m
nats_aus_istfahrt_consumer_backoff_max: 5m

nats_gtfs_rt_stream_name: GTFS_RT_2
nats_gtfs_rt_stream_description: 'GTFS-Realtime FeedEntity messages'
nats_gtfs_rt_stream_subjects:
  - 'gtfsrt.>'
nats_gtfs_rt_stream_max_size_gb: 5

nats_gtfs_rt_consumer_name: nats-consuming-gtfs-rt-server
nats_gtfs_rt_consumer_description: 'OpenDataVBB/nats-consuming-gtfs-rt-server'
nats_gtfs_rt_consumer_deliver: new
nats_gtfs_rt_consumer_max_pending: 500
nats_gtfs_rt_consumer_max_deliver: 3
nats_gtfs_rt_consumer_backoff: linear
nats_gtfs_rt_consumer_backoff_steps: 2
nats_gtfs_rt_consumer_backoff_min: 15s
nats_gtfs_rt_consumer_backoff_max: 1m

# see https://github.com/mholt/caddy-ratelimit/blob/12435ecef5dbb1b137eb68002b85d775a9d5cdb2/README.md
gtfs_rt_rate_limit_requests: 60
gtfs_rt_rate_limit_window: 1m
gtfs_rt_rate_limit_human_readable: '60 requests per minute'

gtfs_rt_webroot: '/var/www/gtfs-rt'

# PostgreSQL
postgresql_major_version: 16 # todo: detect from target!
# password for user `postgres`
postgresql_postgres_password: !vault |
          $ANSIBLE_VAULT;1.1;AES256
          64326365376530323362623666636166336431373238623261613230363435663734643636653534
          3164636564653532663631353863393061616331373233380a373465363630656666613162633064
          32343262343935326531656535323233396333323330653535343365363132333964613337393436
          3438646362363964660a313361636561373237386238383663376163626630393863393233376137
          64333566323161613139306437366239373365323231616465333165663666353230
# number of parallel PostgreSQL queries, effectively
# - the concurrency of *uncached* matchings (defaults to `gtfs_rt_feed_db_pool_size`),
# - plus the concurrency of *uncached* station weight lookups (defaults to `gtfs_rt_feed_db_pool_size`),
# - plus some diagnostic connections (e.g. monitoring, manual debugging).
postgresql_max_connections: '{{ gtfs_rt_feed_db_pool_size * 2 + 3 | string }}'
postgresql_effective_cache_size: '23040MB'
postgresql_shared_buffers: '7680MB'
postgresql_maintenance_work_mem: '1920MB'

# Redis
redis_maxmemory: 4gb
# > Keeps most recently used keys; removes least recently used (LRU) keys
redis_maxmemory_policy: allkeys-lru
gtfs_rt_feed_redis_db: 1

# gtfs-rt-feed
gtfs_rt_feed_git_ref: main # todo: pin?
gtfs_rt_feed_dir: '/srv/gtfs-rt-feed'
# number of parallel PostgreSQL queries, effectively the concurrency of *uncached* matchings
gtfs_rt_feed_db_pool_size: '{{ ansible_processor_nproc * 1.2 | round | int }}' # todo: tweak?
# number of FeedEntities from NATS getting matched, effectively the concurrency of cached & uncached matchings together
gtfs_rt_feed_matching_concurrency: 50
# Schedule days are usually longer than 24 hours: they start a 00:00, but some trips run well beyond 23:59.
gtfs_rt_feed_matching_caching_ttl: 100800 # 1 day 4 hours, in seconds

# gtfs-rt-feed: GTFS import
gtfs_rt_feed_gtfs_importer_db: 'gtfs_importer'
gtfs_rt_feed_gtfs_download_user_agent: '{{ inventory_hostname }} GTFS import'
# This command imports the GTFS data into a database. It then writes a systemd override config file setting $PGDATABASE.
# We assume that it is executed using `sh`.
# todo: make $GTFS_IMPORTER_DB_PREFIX configurable
# todo: redirect logs, set up logrotate
gtfs_rt_feed_import_cmd: "export PGHOST=localhost PGUSER=postgres PGPASSWORD='{{ postgresql_postgres_password }}' PGDATABASE='{{ gtfs_rt_feed_gtfs_importer_db }}' && env GTFS_DOWNLOAD_USER_AGENT='{{ gtfs_rt_feed_gtfs_download_user_agent }}' GTFS_IMPORTER_VERBOSE=true {{ gtfs_rt_feed_dir }}/import.sh --docker && echo \"[Service]\\nEnvironment=PGDATABASE=$(psql -q --csv -t -h localhost -c 'SELECT db_name FROM latest_import')\" | sponge /etc/systemd/system/gtfs-rt-feed.service.d/pgdatabase.conf && systemctl daemon-reload && systemctl restart gtfs-rt-feed.service"

# nats-consuming-gtfs-rt-server
nats_consuming_gtfs_rt_server_git_ref: main # todo: pin?
nats_consuming_gtfs_rt_server_dir: '/srv/nats-consuming-gtfs-rt-server'
# todo: bind only locally? or put behind firewall?
nats_consuming_gtfs_rt_server_internal_port: 3000

# monitoring
gtfs_rt_feed_metrics_server_port: 9091
nats_consuming_gtfs_rt_server_metrics_server_port: 9092
prometheus_listen_address: 'localhost:9090' # only listen locally
prometheus_data_retention: 1y
prometheus_data_retention_size: 20GB
grafana_http_addr: '127.0.0.1' # `localhost` doesn't work
grafana_http_port: 8000
